{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6796c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "from itertools import combinations \n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import surprise\n",
    "import ast\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "\n",
    "#from sklearn.metrics.pairwise import nan_euclidean_distances\n",
    "#!pip install tensorflow\n",
    "#import tensorflow as tf\n",
    "#from sklearn.preprocessing import normalize\n",
    "#from heapq import nlargest\n",
    "#from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3040ae2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  8.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 11.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 13.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 16.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 19.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 22.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 24.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 27.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed: 30.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed: 33.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed: 35.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed: 38.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 40.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed: 90.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed: 92.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed: 95.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed: 98.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 101.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed: 104.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed: 107.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed: 110.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed: 113.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed: 116.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed: 119.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed: 122.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed: 125.0min remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Hyperparameter tuning with cross validation\n",
    "\n",
    "\n",
    "2. Run the model w/ the optimum parameters \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ratings_v2=ratings.copy()\n",
    "data=ratings_v2.rename(\n",
    "    columns={'userId':'userID','movId':'itemID'}).sort_values(\n",
    "    by=['userID','itemID']).drop(columns=['timestamp']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#Split dataset in training and test\n",
    "trainset, testset = train_test_split(data, test_size=0.025, random_state =123)\n",
    "\n",
    "\n",
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = surprise.Reader(rating_scale=( data.rating.min(), data.rating.max() ))\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "trainset = surprise.Dataset.load_from_df(trainset, reader)\n",
    "\n",
    "\n",
    "#Implement Grid Search\n",
    "\"\"\"param_grid = {'n_factors': [2, 5, 10], 'n_epochs': [10, 15, 20],\n",
    "              'biased': [True],\n",
    "              'init_mean': [-1, 0, 1], 'init_std_dev': [0.01],\n",
    "              'lr_bu': [0.005, 0.002], 'lr_bi': [0.005, 0.002],\n",
    "              'lr_pu': [0.005, 0.002], 'lr_qi': [0.005, 0.002],\n",
    "              'reg_bu': [0.05, 0.02, 0.01], 'reg_bi': [0.05, 0.02, 0.01],\n",
    "              'reg_pu': [0.05, 0.02, 0.01], 'reg_qi': [0.05, 0.02, 0.01]\n",
    "             }\"\"\"\n",
    "\n",
    "param_grid = {'n_factors': [2,3,5,7,10], 'n_epochs': [10],\n",
    "              'biased': [True],\n",
    "              'init_mean': [0], 'init_std_dev': [0.01],\n",
    "              'lr_bu': [0.05, 0.02, 0.01], 'lr_bi': [0.05, 0.02, 0.01],\n",
    "              'lr_pu': [0.05, 0.02, 0.01], 'lr_qi': [0.05, 0.02, 0.01],\n",
    "              'reg_bu': [0.1, 0.05, 0.01], 'reg_bi': [0.1, 0.05, 0.01],\n",
    "              'reg_pu': [0.1, 0.05, 0.01], 'reg_qi': [0.1, 0.05, 0.01]\n",
    "             }\n",
    "\n",
    "algo=surprise.prediction_algorithms.matrix_factorization.SVD\n",
    "gs=surprise.model_selection.search.GridSearchCV(algo, param_grid, measures=['mae', 'rmse', 'fcp'], cv=5,\n",
    "                                                refit=False, return_train_measures=True, n_jobs=1,\n",
    "                                                pre_dispatch='2*n_jobs', joblib_verbose=100)\n",
    "\n",
    "display(gs.fit(trainset))\n",
    "error=pd.DataFrame.from_dict(gs.cv_results)\n",
    "for me in ['rmse', 'mae','fcp']:\n",
    "    \n",
    "    print('Error metric: ', me)\n",
    "    print('best score       : ', gs.best_score[me])\n",
    "    print('best parameters  : ', gs.best_params[me], '\\n')\n",
    "    \n",
    "\n",
    "algo = gs.best_estimator[\"mae\"]\n",
    "algo.fit(trainset.build_full_trainset())\n",
    "    \n",
    "\"\"\"#fit the algorothm with the best parameters\n",
    "algoOpt=gs.best_estimator[\"mae\"]\n",
    "modelOpt=surprise.model_selection.validation.cross_validate(algoOpt, data, measures=['mae', 'rmse', 'fcp'], cv=5,\n",
    "                                                            return_train_measures=True, n_jobs=1,\n",
    "                                                            pre_dispatch='2*n_jobs', verbose=True)\n",
    "\n",
    "display(pd.DataFrame.from_dict(modelOpt))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ttest = surprise.Reader(rating_scale=(1,5))\n",
    "ttest = surprise.Dataset.load_from_df(testset, ttest)\n",
    "ttest = ttest.build_testset()\"\"\"\n",
    "\n",
    "\n",
    "algop = gs.best_estimator[\"mae\"]\n",
    "algop.fit(trainset.build_full_trainset())\n",
    "\n",
    "predictions = algop.test(list(testset.itertuples(index=False, name=None)))\n",
    "print('MAE on testset')\n",
    "surprise.accuracy.mae(predictions, verbose=True)\n",
    "\n",
    "predictions = algop.test( list(data[~data.index.isin(testset.index)].itertuples(index=False, name=None)) )\n",
    "print('\\nMAE on trainset')\n",
    "surprise.accuracy.mae(predictions, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f21bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n = get_top_n(predictions, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cef564",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset=ratings_v1.rename(\n",
    "    columns={'userId':'userID','movId':'itemID'}).sort_values(\n",
    "    by=['userID','itemID']).drop(columns=['timestamp']).reset_index(drop=True)\n",
    "\n",
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = surprise.Reader(rating_scale=(1, 5))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = surprise.Dataset.load_from_df(trainset, reader)\n",
    "\n",
    "\n",
    "\n",
    "algo=surprise.prediction_algorithms.matrix_factorization.SVD\n",
    "\n",
    "\n",
    "gs=surprise.model_selection.search.GridSearchCV(algo, param_grid, measures=['rmse', 'mae'], cv=5,\n",
    "                                                refit=True, return_train_measures=True, n_jobs=1,\n",
    "                                                pre_dispatch='2*n_jobs', joblib_verbose=30)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score[\"rmse\"])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b0454",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo1=gs.best_estimator[\"mae\"]\n",
    "algo1.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b331f8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GonçaloVieira\\AppData\\Local\\Temp\\ipykernel_29108\\3844444380.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  users=pd.read_csv(r\"C:\\Users\\GonçaloVieira\\OneDrive - Metyis\\Bureaublad\\Lap\\ML\\datasets\\RS\\medium\\users.dat\",\n",
      "C:\\Users\\GonçaloVieira\\AppData\\Local\\Temp\\ipykernel_29108\\3844444380.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  ratings=pd.read_csv(r\"C:\\Users\\GonçaloVieira\\OneDrive - Metyis\\Bureaublad\\Lap\\ML\\datasets\\RS\\medium\\ratings.dat\",\n",
      "C:\\Users\\GonçaloVieira\\AppData\\Local\\Temp\\ipykernel_29108\\3844444380.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  movies=pd.read_csv(r\"C:\\Users\\GonçaloVieira\\OneDrive - Metyis\\Bureaublad\\Lap\\ML\\datasets\\RS\\medium\\movies.dat\",\n"
     ]
    }
   ],
   "source": [
    "#load datasets\n",
    "\n",
    "users=pd.read_csv(r\"C:\\Users\\GonçaloVieira\\OneDrive - Metyis\\Bureaublad\\Lap\\ML\\datasets\\RS\\medium\\users.dat\",\n",
    "                  sep=\"::\",header=None,names=['userId','gender','age','job','zipcode'])\n",
    "ratings=pd.read_csv(r\"C:\\Users\\GonçaloVieira\\OneDrive - Metyis\\Bureaublad\\Lap\\ML\\datasets\\RS\\medium\\ratings.dat\",\n",
    "                    sep=\"::\",header=None,names=['userId','movieId','rating','timestamp'])\n",
    "movies=pd.read_csv(r\"C:\\Users\\GonçaloVieira\\OneDrive - Metyis\\Bureaublad\\Lap\\ML\\datasets\\RS\\medium\\movies.dat\",\n",
    "                   sep=\"::\",header=None,names=['movieId','title','genres'],encoding='latin-1')\n",
    "movDetails=pd.read_csv(r\"C:\\Users\\GonçaloVieira\\OneDrive - Metyis\\Bureaublad\\Lap\\ML\\datasets\\RS\\webScrapingTMDB\\results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e9b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets Structure\n",
    "\n",
    "print('DATASET users\\n')\n",
    "print(users.info())\n",
    "display(users)\n",
    "\n",
    "print('\\n','*'*50,'\\n')\n",
    "print('DATASET movies\\n')\n",
    "print(movies.info())\n",
    "display(movies)\n",
    "\n",
    "print('\\n','*'*50,'\\n')\n",
    "print('DATASET ratings\\n')\n",
    "print(ratings.info())\n",
    "display(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdad5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EDA (Exploratory Data Analysis)\n",
    "\n",
    "- how many users? how many movies?\n",
    "- how many rated movies? how many tagged movies?\n",
    "- # rated movies per user\n",
    "- # users per rated movies\n",
    "- ratings: median, mean, max, min\n",
    "- unique values of each column, relative and absolute frequency\n",
    "- standard deviation, top 5 & bottom-5 regarding the frequency\n",
    "\n",
    "\"\"\"\n",
    "print('EDA on users dataset\\n')\n",
    "print('num of users            : ',users.userId.nunique())\n",
    "print('userId indexes          : ',users.userId.min() ,'-',users.userId.max())\n",
    "print('users age               : ',users.age.min() ,'-',users.age.max())\n",
    "print('users gender            : ',users.gender.unique())\n",
    "print('num of user jobs        : ',users.job.nunique())\n",
    "print('num of user location    : ',users.zipcode.nunique(),'\\n'*2,'*'*70)\n",
    "\n",
    "print('\\nEDA on movies dataset\\n')\n",
    "print('num of movies           : ',movies.movieId.nunique())\n",
    "print('movieID indexes         : ',movies.movieId.min() ,'-',movies.movieId.max())\n",
    "print('movie release year      : ',\n",
    "      movies.title.str.slice(-5,-1).astype(int).min() ,'-',movies.title.str.slice(-5,-1).astype(int).max(),\n",
    "     '\\n'*2,'*'*70)\n",
    "\n",
    "print('\\nEDA on ratings dataset\\n')\n",
    "print('num of users that rated : ',ratings.userId.nunique())\n",
    "print('num of rated movies     : ',ratings.movieId.nunique())\n",
    "print('rating values           : ',np.sort(ratings.rating.unique()) )\n",
    "\n",
    "aux=ratings.groupby(by=['userId','movieId']).rating.count().reset_index().sort_values('rating',ascending=False)\n",
    "print('Number of users that rated the same movie multiple times: ',aux[aux.rating>1].userId.nunique(),'users')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d09f0",
   "metadata": {},
   "source": [
    "**Transformations**\n",
    "\n",
    "+ **Movies Dataset (movieId, title, genres)**\n",
    "    \n",
    "    > Create a variable indicating the number of ratings and order the dataset in\n",
    "    descending order considering this variable.\n",
    "    \n",
    "    > Create a similar feature to movieId named movId. This variable will be used\n",
    "    in the model due to convenience\n",
    "    \n",
    "+ **Movies Details Dataset**\n",
    "\n",
    "    > Through hot encoding, add new columns with all movie genres\n",
    "    \n",
    "    \n",
    "+ **Ratings Dataset (userId, movieId, rating, timestamp )**\n",
    "\n",
    "    > Add movId feature to this dataset\n",
    "    \n",
    "    > Format timestamp feature\n",
    "    \n",
    "    > Format userId feature\n",
    "    \n",
    "+ **Users Dataset (userId, gender, age, job, zipcode )**\n",
    "\n",
    "    > Format userId feature\n",
    "    \n",
    "    > Add feature especifing the type of job a certain user has, jobT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ca901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux=ratings.groupby('movieId').userId.count().reset_index()\n",
    "aux.rename(columns={'userId':'nRatings'},inplace=True)\n",
    "movies=movies.merge(aux, how='left',on='movieId')\n",
    "movies.nRatings.fillna(0,inplace=True)\n",
    "movies.nRatings=movies.nRatings.astype(int)\n",
    "movies.sort_values(by=['nRatings'],ascending=[False],inplace=True)\n",
    "movies.reset_index(inplace=True)\n",
    "movies['movId']=movies.index\n",
    "movies=movies[['movieId','movId','title','nRatings','genres']]\n",
    "#movies[['movieId','movId','title','genres']].to_csv(r\"C:\\Users\\GonçaloVieira\\OneDrive - Metyis\\Bureaublad\\Lap\\ML\\datasets\\RS\\webScrapingTMDB\\moviesMed.csv\")\n",
    "\n",
    "\n",
    "\n",
    "movDetails.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "movDetails.crew=movDetails.crew.apply(ast.literal_eval)\n",
    "movDetails.cast=movDetails.cast.apply(ast.literal_eval)\n",
    "movDetails.tags=movDetails.tags.apply(lambda x: x[1:-1].replace(\"'\",\"\").split(','))\n",
    "movDetails=movDetails.merge(movies[['movId','title']], how='left', left_on='titleM',right_on='title')\n",
    "movDetails.drop(columns=['title_y'], inplace=True)\n",
    "movDetails.rename(columns={'title_x':'title'}, inplace=True)\n",
    "\n",
    "aux=movies['genres'].str.split('|',expand=True)\n",
    "aux['title']=movies.title\n",
    "aux=pd.melt(aux,id_vars='title',value_name='genre')\n",
    "aux=aux[['title','genre']]\n",
    "aux.dropna(inplace=True)\n",
    "aux['val']=1\n",
    "aux=aux.set_index(['title','genre']).val.unstack().fillna(0)\n",
    "display\n",
    "movDetails=movDetails.merge(aux, how='left', left_on='titleM',right_on=aux.index)\n",
    "\n",
    "\n",
    "\n",
    "ratings=ratings.merge(movies[['movieId','movId']],how='left', on='movieId')\n",
    "ratings=ratings[['userId','movId','rating','timestamp']]\n",
    "ratings['timestamp'] = pd.to_datetime(ratings['timestamp'],unit='s')\n",
    "ratings.userId=ratings.userId-1\n",
    "ratings.sort_values(by=['userId','movId'], inplace=True)\n",
    "ratings.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "users.userId=users.userId-1\n",
    "aux={\n",
    "    0:  \"other or not specified\",\n",
    "    1:  \"academic/educator\",\n",
    "    2:  \"artist\",\n",
    "    3:  \"clerical/admin\",\n",
    "    4:  \"college/grad student\",\n",
    "    5:  \"customer service\",\n",
    "    6:  \"doctor/health care\",\n",
    "    7:  \"executive/managerial\",\n",
    "    8:  \"farmer\",\n",
    "    9:  \"homemaker\",\n",
    "    10:  \"K-12 student\",\n",
    "    11:  \"lawyer\",\n",
    "    12:  \"programmer\",\n",
    "    13:  \"retired\",\n",
    "    14:  \"sales/marketing\",\n",
    "    15:  \"scientist\",\n",
    "    16:  \"self-employed\",\n",
    "    17:  \"technician/engineer\",\n",
    "    18:  \"tradesman/craftsman\",\n",
    "    19:  \"unemployed\",\n",
    "    20:  \"writer\",\n",
    "}\n",
    "users=users.merge(pd.DataFrame(aux.items(),columns=['job','jobT']), how='left', on='job')\n",
    "aux={\n",
    "    1:  \"Under 18\",\n",
    "    18:  \"18-24\",\n",
    "    25:  \"25-34\",\n",
    "    35:  \"35-44\",\n",
    "    45:  \"45-49\",\n",
    "    50:  \"50-55\",\n",
    "    56:  \"56+\"\n",
    "}\n",
    "users=users.merge(pd.DataFrame(aux.items(),columns=['age','ageT']), how='left', on='age')\n",
    "users=users[['userId', 'gender','age','ageT','job','jobT','zipcode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2435a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EDA (Exploratory Data Analysis)\n",
    "Double Check\n",
    "\n",
    "    - Number of distinct movies matches movie lowest index and movie greatest index\n",
    "    - Number of distinct users matches user lowest index and user greatest index\n",
    "    \n",
    "\"\"\"\n",
    "print('Double Checks\\n')\n",
    "print('dc #1:\\n','# unique movies: ',movies.movieId.nunique(),'// indexes:',movies.movId.min(),'-',movies.movId.max())\n",
    "print('dc 21:\\n','# unique users : ',ratings.userId.nunique(),' // indexes:',ratings.userId.min(),'-',ratings.userId.max())\n",
    "\n",
    "\n",
    "print('Stats after transformations:')\n",
    "print('\\nNum of ratings per user')\n",
    "display(ratings.groupby('userId').movId.count().describe().round())\n",
    "\n",
    "print('\\nNum of ratings per movie')\n",
    "display(ratings.groupby('movId').userId.count().describe().round())\n",
    "\n",
    "print('\\nratings - summary stats')\n",
    "display(ratings.rating.describe().round(2))\n",
    "\n",
    "\n",
    "\"\"\"print('\\nMost seen movies per category')\n",
    "display(\n",
    "    ratings.merge(movies.loc[:,['movId']+list(movies.columns[-18:])] ,how='left',\n",
    "             on='movId').iloc[:,4:].sum(axis=0).sort_values(ascending=False)/ratings.shape[0]*100\n",
    ")\n",
    "\n",
    "print('\\nMost seen movies per pairs of categories')\n",
    "aux=ratings_v1.merge(movies.loc[:,['movId']+list(movies.columns[-18:])], how='left',\n",
    "                  on='movId')\n",
    "aux1=[]\n",
    "for p in combinations(movies.drop(columns=movies.columns[:-18]).columns,2):\n",
    "    aux1.append([p[0],p[1],aux[(aux[p[0]]==1) & (aux[p[1]]==1)].shape[0]])\n",
    "\n",
    "display(pd.DataFrame(aux1,columns=['genre1','genre2','freq']).sort_values('freq',ascending=False).head(10))\"\"\"\n",
    "\n",
    "print('\\nMost popular movies')\n",
    "display( movies[['movId','title','nRatings']].sort_values(by='nRatings',ascending=False).head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b437726",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux=ratings[['userId','movId','rating']].copy()\n",
    "\n",
    "aux=aux.groupby('movId').userId.count().reset_index()\n",
    "aux.rename(columns={'userId':'nRatings'},inplace=True)\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.histplot(data=aux.nRatings, bins=30, kde=False)\n",
    "axs.set(xlabel='Number of ratings', ylabel='Number of movies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "aux=ratings[ratings.rating>=4][['userId','movId']]\n",
    "aux=aux.merge(movies[['movId','title']], on='movId', how='left')\n",
    "aux['hotEncode']=True\n",
    "aux.set_index(['userId','title'])['hotEncode'].unstack().fillna(False)\n",
    "\n",
    "assocRules=association_rules(\n",
    "    fpgrowth(aux.set_index(['userId','title'])['hotEncode'].unstack().fillna(False),\n",
    "         min_support=0.10,\n",
    "         use_colnames=True),\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=0.70)\n",
    "assocRules.antecedents=assocRules.antecedents.astype(str).str.slice(start=11, stop=-2)\n",
    "assocRules.consequents=assocRules.consequents.astype(str).str.slice(start=11, stop=-2)\n",
    "\n",
    "assocRules.to_csv(r\"C:\\Users\\GonçaloVieira\\OneDrive - Metyis\\Bureaublad\\Lap\\ML\\datasets\\RS\\EDA\\AssociationRules\\assocRules.csv\")\n",
    "assocRules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Realizar um cluster considerando os generos de filmes\n",
    "Para cada user determinar a porçao de ratings (>=4) para cada genero de filme\n",
    "\n",
    "    > there are 2 users whose ratings are all less than 4\n",
    "    set(users.userId)-set(clUsers.index)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "clUsers=ratings.merge(movDetails.iloc[:,14:], on='movId', how='left').drop(columns=['timestamp'])\n",
    "clUsers=clUsers[clUsers.rating>=4].drop(columns=['movId', 'rating'])\n",
    "clUsers=clUsers.groupby(by=['userId']).sum()\n",
    "clUsers=clUsers/np.sum(np.array(clUsers), axis=1, keepdims=True)\n",
    "display(clUsers)\n",
    "\n",
    "def kmeansModel(data,k):\n",
    "    \"\"\"\n",
    "    inertia = sum of squared distances between each point and the closest centroid\n",
    "    w_in    = sum of intra cluster distances\n",
    "    w_out   = sum of inter cluster distances\n",
    "\n",
    "    N_in    = num of intra cluster edges\n",
    "    N_out   = num of inter cluster edges\n",
    "    \n",
    "    silhouette_score can be calculated in the following way:\n",
    "    > silhouette_score(model_1n.iloc[:,2:], kmeans.labels_, metric='euclidean')    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10).fit(np.array(data))\n",
    "    #print(kmeans.cluster_centers_)\n",
    "    #print('inertia: ',kmeans.inertia_)\n",
    "    aux=data.copy()\n",
    "    aux['label']=kmeans.labels_\n",
    "\n",
    "    inertia=0\n",
    "    w_in=0\n",
    "    w_out=0\n",
    "    N_in=0\n",
    "    N_out=0\n",
    "\n",
    "    for j in range(k):\n",
    "\n",
    "        inertia = inertia + np.sum((np.array(aux[aux.label==j].iloc[:,:-1])-kmeans.cluster_centers_[j])**2)\n",
    "        w_in=w_in+np.sum(distance_matrix(aux[aux.label==j].iloc[:,:-1],aux[aux.label==j].iloc[:,:-1],p=2))\n",
    "        w_out=w_out+np.sum(distance_matrix(aux[aux.label==j].iloc[:,:-1],aux[aux.label!=j].iloc[:,:-1],p=2))\n",
    "\n",
    "        N_in=N_in+( aux[aux.label==j].shape[0] * (aux[aux.label==j].shape[0]-1) )\n",
    "        N_out=N_out+( aux[aux.label==j].shape[0] * aux[aux.label!=j].shape[0] )\n",
    "\n",
    "    w_in=w_in/2\n",
    "    w_out=w_out/2\n",
    "    N_in=int(N_in/2)\n",
    "    N_out=int(N_out/2)\n",
    "\n",
    "    #print('inertia : ', inertia)\n",
    "    #print('w_in    : ', w_in)\n",
    "    #print('w_out   : ', w_out)\n",
    "    #print('N_in    : ', N_in)\n",
    "    #print('N_out   : ', N_out)\n",
    "    #print('BetaCV  : ', (w_in/N_in)/(w_out/N_out))\n",
    "    \n",
    "    \n",
    "    \"\"\"# Compute Gap Statistic\n",
    "    feats=list(aux.columns[:-1])\n",
    "    w_inR_values=[]\n",
    "    for t in range(100):\n",
    "        R=[]\n",
    "        for c in feats:\n",
    "            R.append(np.random.uniform(aux[c].min(),aux[c].max(),data.shape[0]))\n",
    "        R=pd.DataFrame(np.array(R).T,columns=feats)\n",
    "        R['label']=KMeans(n_clusters=k, init='k-means++', n_init=10).fit(R).labels_\n",
    "        \n",
    "        w_inR=0\n",
    "        for j in range(k):\n",
    "            w_inR=w_inR+np.sum(distance_matrix(R[R.label==j].iloc[:,:-1],R[R.label==j].iloc[:,:-1],p=2))\n",
    "        w_inR_values.append(w_inR)\n",
    "    mean_w_in=np.mean(np.log2(w_inR_values))\n",
    "    std_w_in =np.std(np.log2(w_inR_values))\n",
    "    gap=mean_w_in-np.log2(w_in)\"\"\"\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    return (inertia,w_in,w_out,N_in,N_out,(w_in/N_in)/(w_out/N_out),\n",
    "            np.mean(silhouette_samples(data, kmeans.labels_, metric='euclidean')),\n",
    "            silhouette_samples(data, kmeans.labels_, metric='euclidean'),\n",
    "            #gap, std_w_in, \n",
    "            aux)\n",
    "\n",
    "\n",
    "# 2. K-Means\n",
    "\n",
    "nclusters=range(2,19)\n",
    "w_in=[]\n",
    "betaCV=[]\n",
    "silhouette_score=[]\n",
    "gap=[]\n",
    "gap_std=[]\n",
    "\n",
    "for j in nclusters:\n",
    "    \n",
    "    model=kmeansModel(clUsers,j)\n",
    "    w_in.append(model[1])\n",
    "    betaCV.append(model[5])\n",
    "    silhouette_score.append(model[6])\n",
    "    #gap.append(model[8])\n",
    "    #gap_std.append(model[9])\n",
    "\n",
    "fig,axs=plt.subplots(1,3,figsize=(30, 10))\n",
    "axs[0].plot(nclusters, w_in, '-o')\n",
    "axs[0].set_title('Intra Cluster Distance')\n",
    "axs[1].plot(nclusters, betaCV, '-o')\n",
    "axs[1].set_title('BetaCV')\n",
    "axs[2].plot(nclusters, silhouette_score, '-o')\n",
    "axs[2].set_title('Silhouette Score')\n",
    "\"\"\"axs[1,1].errorbar(nclusters, gap, gap_std, \n",
    "             linestyle='-', marker='o',\n",
    "            ecolor='red',capsize=10)\n",
    "axs[1,1].set_title('Gap Statistic')\"\"\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad652fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Results from the clustering\n",
    "k=5, 7 or 8\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "kmeans = KMeans(n_clusters=8, init='k-means++', n_init=10, random_state=123).fit(np.array(clUsers))\n",
    "clUsers['label']=kmeans.labels_\n",
    "clUsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f832ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lab in clUsers.label.unique():\n",
    "    \n",
    "    print('cluster #',lab)\n",
    "    aux=clUsers[clUsers.label==lab]\n",
    "    print('cluster size: ', aux.shape[0])\n",
    "    print(aux.iloc[:,:-1].sum(axis=0).sort_values(ascending=False).head(3),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95868bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1,2],[3,4],[5,6]]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1426b7e",
   "metadata": {},
   "source": [
    "\n",
    "**Model**\n",
    "\n",
    "**A. Collab Filtering**\n",
    "    \n",
    "    1. Hold Out : Split data in 3 datasets\n",
    "        - training     95%\n",
    "        - validation   2.5%\n",
    "        - test         2.5%\n",
    "    \n",
    "    2. Algorithm:\n",
    "        2.1 Obtain similarity matrix among users\n",
    "            [* To compare 2 users, they must have a minimum number of items in common\n",
    "            ( ncm is used as a parameter ) pd.DataFrame.corr(method='pearson', min_periods=ncm)]\n",
    "            \n",
    "        2.2 Compute the neighborhood of each user\n",
    "        \n",
    "            2.2.1 For each target user his neighborhood must satisfy the following conditions:\n",
    "                ** Each neighbor must be similar enough to the target user, accessed by min_sim\n",
    "                ** The whole neighborhood must have exactly nnei neighbors that respect the \n",
    "                previous condition, excluding the target user\n",
    "                \n",
    "                For users in the such conditions the Collab Filtering algorithm will predict \n",
    "                recommendations.\n",
    "                The other users will have recommendations based on Naive Bayes CF.\n",
    "                \n",
    "                It is not necessary that the whole neighborhood contributes to the prediction\n",
    "                of a movie. It can happen, a recommendation for a certain user came from only 1 of \n",
    "                his neighbors. Note also that a user may not have a neighborhood but can be \n",
    "                in some neighborhood. Therefore, even though CF may not be able to recommend items \n",
    "                to certain user, the user's ratings can be used to recommend items to other users.\n",
    "                \n",
    "                \n",
    "            \n",
    "        2.3 Compute items - potencial recommendations, for each user\n",
    "        2.4 Predict ratings for the potencial recommendation items\n",
    "         \n",
    "    3. Access avaliation metrics on all predicted movies and top-5 recommendations per user\n",
    "        - Hit Rate (%)\n",
    "        'Hit Rate (%)': np.round(ratingsPredValid.shape[0]/dvaldCV[dvaldCV.userId.isin(cfUsers)].shape[0]*100,1)\n",
    "        \n",
    "        - Precision (RMSE, MAE, NRMSE, NMAE)\n",
    "        - Coverage + Diversity\n",
    "        - Novelty + Serendipity\n",
    "        \n",
    "        \n",
    "**To do**\n",
    "\n",
    "    - rmatrix pode conter 0s por causa de nao haver rating, ie, os users nao viram\n",
    "    um determinado filme OU entao por que resulta de r_uj:=r_uj-media_u\n",
    "    ESTE problema esta em >>> rmatrix_n_neiu_predm!=0\n",
    "    MESMO problema em mov_pred (ja resolvido!!?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f71d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSplit(ratings,ptest):\n",
    "    \n",
    "    \"\"\"\n",
    "    Split ratings dataset in 3 smaller datasets:\n",
    "        - Training\n",
    "        - Validation\n",
    "        - Test\n",
    "    Note that\n",
    "        - validation dataset will be the same size as test dataset\n",
    "        - Hold Out or Cross validation \n",
    "        - validation dataset is within training dataset   \"\"\"\n",
    "    \n",
    "    dtrain,dtest=train_test_split(ratings,\n",
    "                       test_size=ptest, random_state=123, shuffle=True)\n",
    "    dtest.sort_values(by=['userId','movId'],inplace=True)\n",
    "    dtrain.sort_values(by=['userId','movId'],inplace=True)\n",
    "    \n",
    "    cv=np.repeat( range(1,int(ptest**(-1))+1), np.floor(dtrain.shape[0]*ptest) )\n",
    "    cv=np.concatenate((cv,np.random.choice(range(1,int(ptest**(-1))+1), size=dtrain.shape[0]-cv.shape[0], replace=False)))\n",
    "    np.random.shuffle(cv)\n",
    "    dtrain['cvSplit']=cv\n",
    "    \n",
    "    aux=ratings.groupby('movId').userId.count().reset_index().merge(\n",
    "        dtrain.groupby('movId').userId.count().reset_index(), how='left', on='movId')\n",
    "    print('Which movie contributes the most in the test dataset? or\\nthe highest % of ratings used from a certain movie in the test dataset is :',\n",
    "          np.round(100-(aux.userId_y/aux.userId_x*100).min(),1),'%')\n",
    "    aux=ratings.groupby('userId').movId.count().reset_index().merge(\n",
    "        dtrain.groupby('userId').movId.count().reset_index(), how='left', on='userId')\n",
    "    print('Which user contributes the most in the test dataset? or\\nthe highest % of ratings used from a certain user in the test dataset is  :',\n",
    "          np.round(100-(aux.movId_y/aux.movId_x*100).min(),1),'%\\n')\n",
    "    \n",
    "    return dtrain,dtest\n",
    "\n",
    "\n",
    "def simMatrix(ratings, mncmr):\n",
    "    \n",
    "    start=time.time()\n",
    "    #compute similarity matrix using surprise package\n",
    "    aux=ratings.copy()\n",
    "    aux['userRat'] = list(zip(aux.userId, aux.rating))\n",
    "    simUsers={}\n",
    "\n",
    "    for m in np.sort(aux.movId.unique()):\n",
    "        simUsers[m]=list(aux[aux.movId==m].userRat)\n",
    "        \n",
    "    simUsers=surprise.similarities.msd(aux.userId.nunique(), simUsers, min_support=mncmr)\n",
    "    np.fill_diagonal(simUsers, 0)\n",
    "        \n",
    "    print('Similarity matrix computation time (secs): ', np.round(time.time()-start,1))\n",
    "    return simUsers\n",
    "    \n",
    "    \n",
    "def neighborhood(sim, nnei, simMin):\n",
    "    \n",
    "    \"\"\"\n",
    "    2.2 Compute neighborhoods & exclude non correlated neighbors.\n",
    "    Compute users where CF will be applied. \"\"\"\n",
    "    nu=sim.shape[0]\n",
    "    start=time.time()\n",
    "    viz=[[np.sort(np.argpartition(sim[u], -nnei)[-nnei:]) \n",
    "         ,sim[u][np.sort(np.argpartition(sim[u], -nnei)[-nnei:])]]\n",
    "                  \n",
    "         for u in range(nu)]\n",
    "    \n",
    "    #exclude non correlated neighbors\n",
    "    viz=[viz[u][0][np.where(viz[u][1]>=simMin)]\n",
    "         for u in range(nu)]\n",
    "    \n",
    "    cfUsers=np.where(np.array([len(v) for v in viz])>=nnei)[0]\n",
    "    ncfUsers=list(set(range(nu))-set(cfUsers))\n",
    "    \n",
    "    print('Neighborhood computation time (secs)     : ',np.round(time.time()-start,1))\n",
    "    \n",
    "    return viz, cfUsers, ncfUsers\n",
    "    \n",
    "def itemsToPredict(rmatrix, viz, cfUsers):\n",
    "    \n",
    "    start=time.time()\n",
    "    movPred=[np.sort(list(set(np.where((~np.isnan(rmatrix[viz[u],:])).sum(axis=0)!=0)[-1]) -\n",
    "                           set(np.where(~np.isnan(rmatrix[u,:]))[-1]) \n",
    "                          ))\n",
    "              for u in cfUsers]\n",
    "\n",
    "    print('Movies to predict computation time (secs): ',np.round(time.time()-start,1))\n",
    "    \n",
    "    return movPred\n",
    "\n",
    "\n",
    "def itemsPredictions(rmatrixMC, cfUsers, movPred, viz, sim, rmatrix):\n",
    "    \n",
    "    start=time.time()\n",
    "    pred=[]\n",
    "\n",
    "    rmatrixMC0=rmatrixMC.copy()\n",
    "    rmatrixMC0[np.isnan(rmatrixMC0)] = 0\n",
    "    for u,v in zip(cfUsers, range(len(movPred))):\n",
    "\n",
    "        rmatrixMC0_NeiuPredm=rmatrixMC0[viz[u]][:,movPred[v]]\n",
    "        simUNeiu=sim[u,viz[u]]\n",
    "\n",
    "        predU=np.matmul(simUNeiu,\n",
    "                        rmatrixMC0_NeiuPredm)/np.matmul(np.abs(simUNeiu),\n",
    "                                                        ~np.isnan(rmatrix[viz[u]][:,movPred[v]]))\n",
    "        \n",
    "        pred.append(predU)\n",
    "        \n",
    "    pred=[p+np.nanmean(rmatrix[u]) for p,u in zip(pred,cfUsers)]\n",
    "    print('Prediction computation time (secs)       : ', np.round(time.time()-start,1),'\\n')\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def collabFiltering(sim, nnei, simMin, rmatrix, rmatrixMC):          \n",
    "\n",
    "    # 2.2 Compute neighborhoods and cfUsers\n",
    "    viz, cfUsers, ncfUsers = neighborhood(sim, nnei, simMin)\n",
    "\n",
    "    # 2.3 Compute movies to predict\n",
    "    movPred=itemsToPredict(rmatrix, viz, cfUsers)\n",
    "\n",
    "    # 2.4 Predict ratings and get top nri recommendations\n",
    "    pred =itemsPredictions(rmatrixMC, cfUsers, movPred, viz, sim, rmatrix)\n",
    "    \n",
    "    return pd.DataFrame({'userId':np.repeat(cfUsers,[len(mo) for mo in movPred]),\n",
    "                         'movId':np.hstack(movPred), 'ratingPred':np.hstack(pred)}), cfUsers, ncfUsers\n",
    "    \n",
    "#ola=simMatrix(rmatrixMC, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dddaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rs_cf(ratings, ptest, crossValid):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    \n",
    "    crossvalid=True or False          # whether or not to perform Cross Validation\n",
    "    mncmr=37                          # number of common items (movies)\n",
    "    simMin=0.3                        # minimum similarity between users to be in a neighborhood\n",
    "    nnei=45                           # number of neighborhoods \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #Split original dataset in training and test\n",
    "    dtrain, dtest = dataSplit(ratings,ptest)\n",
    "    \n",
    "    if crossValid==True:\n",
    "        \n",
    "        erro={}\n",
    "        for mncmr in np.arange(45,50,5):\n",
    "            for nnei in np.arange(40,50,5):\n",
    "                for simMin in np.arange(0.5,0.6,0.1):\n",
    "                    \n",
    "                    NRMSE=[]\n",
    "                    NMAE=[]\n",
    "                    for i in np.sort(dtrain.cvSplit.unique()):\n",
    "\n",
    "                        dvaldCV=dtrain[dtrain.cvSplit==i]\n",
    "                        dtrainCV=dtrain[dtrain.cvSplit!=i]\n",
    "\n",
    "                        # 2.1 Compute rating matrix and similarity matrix\n",
    "                        rmatrix   = np.array(dtrainCV.set_index(['userId','movId']).rating.unstack())\n",
    "                        rmatrixMC = rmatrix-np.nanmean(rmatrix, axis=1, keepdims=True)\n",
    "                        sim=simMatrix(dtrainCV, mncmr)            \n",
    "\n",
    "                        # 2.234 Collaborative Filtering\n",
    "                        pred, cfUsers, ncfUsers =collabFiltering(sim, nnei, simMin, rmatrix, rmatrixMC)\n",
    "\n",
    "                        # Access the prediction quality on the validation set\n",
    "                        predVald=pred.merge(dvaldCV[['userId','movId','rating']], how='inner', on=['userId','movId'])\n",
    "                        predVald.ratingPred=np.where(predVald.ratingPred>5,5,\n",
    "                                                     np.where(predVald.ratingPred<1,1,predVald.ratingPred))\n",
    "\n",
    "                        erroi={'% of users w/ ratings': np.round(100*len(cfUsers)/(len(cfUsers)+len(ncfUsers)),1),\n",
    "                               'Normalized RMSE (NRMSE %)': np.round(\n",
    "                                   100*np.sqrt(np.sum((predVald.ratingPred-predVald.rating)**2)/predVald.shape[0])/5,\n",
    "                                   1),\n",
    "                               'Normalized MAE (NMAE %)': np.round(\n",
    "                                   100*np.sqrt(np.sum(np.abs(predVald.ratingPred-predVald.rating))/predVald.shape[0])/5,\n",
    "                                   1)\n",
    "                              }\n",
    "                        NRMSE.append(erroi['Normalized RMSE (NRMSE %)'])\n",
    "                        NMAE.append(erroi['Normalized MAE (NMAE %)'])\n",
    "\n",
    "                    erroP={'Normalized RMSE (NRMSE %)': np.mean(NRMSE),\n",
    "                           'Normalized MAE (NMAE %)': np.mean(NMAE)}\n",
    "                    erro[(mncmr, nnei, simMin)]=erroP\n",
    "        \n",
    "        return dtrain, dtest, erro\n",
    "    \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #Split training dataset in training and validation\n",
    "        dvald=dtrain[dtrain.cvSplit==np.random.choice(dtrain.cvSplit.unique(), size=1, replace=False)[0] ]\n",
    "        dtrain=dtrain[dtrain.cvSplit!=dvald.cvSplit.unique()[0]]\n",
    "        \n",
    "        # 2.1 Compute rating matrix\n",
    "        rmatrix   = np.array(dtrain.set_index(['userId','movId']).rating.unstack())\n",
    "        rmatrixMC = rmatrix-np.nanmean(rmatrix, axis=1, keepdims=True)\n",
    "        \n",
    "        erro={}\n",
    "        for mncmr in np.arange(45,50,5):\n",
    "            for nnei in np.arange(40,50,5):\n",
    "                for simMin in np.arange(0.5,0.6,0.1):\n",
    "                    \n",
    "                    # 2.1 Compute similarity matrix\n",
    "                    sim=simMatrix(dtrain, mncmr)            \n",
    "\n",
    "                    # 2.234 Collaborative Filtering\n",
    "                    pred, cfUsers, ncfUsers =collabFiltering(sim, nnei, simMin, rmatrix, rmatrixMC)\n",
    "\n",
    "                    # Access the prediction quality on the validation set\n",
    "                    hitRate=dvald[dvald.rating>=4].merge(\n",
    "                        pred[pred.ratingPred>=4.5],\n",
    "                        how='inner', on=['userId','movId']).sort_values(by=['userId','movId'])\n",
    "                    \n",
    "                    predVald=pred.merge(dvald[['userId','movId','rating']], how='inner', on=['userId','movId'])\n",
    "                    \n",
    "                    predVald.ratingPred=np.where(predVald.ratingPred>5,5,\n",
    "                                                 np.where(predVald.ratingPred<1,1,predVald.ratingPred))\n",
    "\n",
    "                    erroP={'% of users w/ ratings': np.round(100*len(cfUsers)/(len(cfUsers)+len(ncfUsers)),1),\n",
    "                           'Normalized RMSE (NRMSE %)': np.round(\n",
    "                               100*np.sqrt(np.sum((predVald.ratingPred-predVald.rating)**2)/predVald.shape[0])/5,\n",
    "                               1),\n",
    "                           'Normalized MAE (NMAE %)': np.round(\n",
    "                               100*np.sqrt(np.sum(np.abs(predVald.ratingPred-predVald.rating))/predVald.shape[0])/5,\n",
    "                               1),\n",
    "                           'Hit Rate (%)': np.round(100*hitRate.userId.nunique()/len(cfUsers))\n",
    "                          }\n",
    "                    erro[(mncmr, nnei, simMin)]=erroP\n",
    "\n",
    "        return dtrain, dvald, dtest, erro\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91971b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtrain, dvald, dtest, erro = rs_cf(ratings_v1, 0.025, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2284b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.ratingPred=np.where(pred.ratingPred>5,5,pred.ratingPred)\n",
    "pred.merge(dtest, how='inner',on=['userId','movId']).sort_values(['ratingPred','rating'],ascending=[False,True]).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsPred_t=pred.merge(dtest[['userId','movId','rating']],how='inner',on=['userId','movId'])\n",
    "ratingsPred_t.ratingPred=np.where(ratingsPred_t.ratingPred>5,5,\n",
    "                                     np.where(ratingsPred_t.ratingPred<1,1,ratingsPred_t.ratingPred))\n",
    "print('Normalized RMSE (NRMSE):\\n',\n",
    "      np.round(\n",
    "          100*np.sqrt(np.sum((ratingsPred_t.ratingPred-ratingsPred_t.rating)**2)/ratingsPred_t.shape[0])/5 , 1) )\n",
    "print('Normalized MAE (NMAE):\\n',\n",
    "      np.round(\n",
    "          100*np.sqrt(np.sum(np.abs(ratingsPred_t.ratingPred-ratingsPred_t.rating))/ratingsPred_t.shape[0])/5, 1) )\n",
    "print('Hit Rate:\\n',\n",
    "      np.round(ratingsPred_t.shape[0]/dtest[dtest.userId.isin(usersdiv['cf_users'])].shape[0]*100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56290f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "NAIVE BAYES Algorithm\n",
    "for cb_users\n",
    "\n",
    "\n",
    "\n",
    "ratings_v2=ratings.copy()\n",
    "ratings_v2=ratings_v2.merge(movies[['movieId','movId']],how='left',\n",
    "                            on='movieId')\n",
    "ratings_v2=ratings_v2[['userId','movId','rating','timestamp']]\n",
    "ratings_v2['timestamp'] = pd.to_datetime(ratings_v2['timestamp'],unit='s')\n",
    "ratings_v2.userId=ratings_v2.userId-1\n",
    "ratings_v2=ratings_v2[~ratings_v2.index.isin(dtest.index+dvald.index)]\n",
    "\n",
    "\n",
    "#Split in training and test\n",
    "ptest=2.5/100\n",
    "aux=ratings_v2[ratings_v2.userId.isin(usersdiv['ncf_users'])]\n",
    "dtest=aux.sample(n=int(ptest*aux.shape[0]),replace=False,random_state=123)\n",
    "dtest.sort_index(inplace=True)\n",
    "\n",
    "#Split in training and validation\n",
    "dtrain=ratings_v2[~(ratings_v2.index.isin(dtest.index))]\n",
    "pvald=2.5/100\n",
    "dvald=dtrain[dtrain.userId.isin(usersdiv['ncf_users'])].sample(n=int(pvald*aux.shape[0]), replace=False,random_state=123)\n",
    "dvald.sort_index(inplace=True)\n",
    "dtrain=dtrain[~(dtrain.index.isin(dvald.index))]\n",
    "\n",
    "aux=ratings_v2[ratings_v2.userId.isin(usersdiv['ncf_users'])].groupby('movId').userId.count().reset_index().merge(\n",
    "    dtrain[dtrain.userId.isin(usersdiv['ncf_users'])].groupby('movId').userId.count().reset_index(), how='left', on='movId')\n",
    "print('Which movie contributes the most in the test dataset? or\\nthe highest % of ratings used from a certain movie in the test dataset is:',\n",
    "      np.round(100-(aux.userId_y/aux.userId_x*100).min(),1),'%')\n",
    "aux=ratings_v2[ratings_v2.userId.isin(usersdiv['ncf_users'])].groupby('userId').movId.count().reset_index().merge(\n",
    "    dtrain[dtrain.userId.isin(usersdiv['ncf_users'])].groupby('userId').movId.count().reset_index(), how='left', on='userId')\n",
    "print('Which user contributes the most in the test dataset? or\\nthe highest % of ratings used from a certain user in the test dataset is:',\n",
    "      np.round(100-(aux.movId_y/aux.movId_x*100).min(),1),'%')\n",
    "\n",
    "nbP1=dtrain.copy()\n",
    "nbP1['count']=1\n",
    "nbP1=nbP1.groupby(by=['movId','rating'])['count'].sum().reset_index().sort_values(by=['movId','rating'])\n",
    "nbP1=nbP1.pivot(index='movId', columns='rating', values='count').fillna(0)\n",
    "\n",
    "#movies to predict\n",
    "movies_pred=[]\n",
    "movies_seen=[]\n",
    "mov=set(dtrain.movId.unique())\n",
    "for u in usersdiv['ncf_users']:\n",
    "    movies_pred.append(list(mov-set(dtrain[dtrain.userId==u].movId.unique())))\n",
    "    movies_seen.append(dtrain[dtrain.userId==u].movId.unique())\n",
    "    \n",
    "#predictions\n",
    "alpha=1\n",
    "l=5\n",
    "nbP1=(nbP1+alpha).div(nbP1.sum(axis=1)+l*alpha,axis=0)\n",
    "nbP1a=np.array(nbP1)\n",
    "\n",
    "ratMov=dtrain.sort_values(by=['movId','rating'])\n",
    "ratMov['ratMovId']=(ratMov.rating.astype(str)+ratMov.movId.astype(str))#.astype(int)\n",
    "ratMov=ratMov.ratMovId.unique()\n",
    "nbP2=pd.DataFrame(np.zeros( (ratMov.shape[0],ratMov.shape[0]) ),columns=ratMov)\n",
    "nbP2.index=ratMov\n",
    "start=time.time()\n",
    "for rm in ratMov:\n",
    "    \n",
    "    m=int(str(rm)[1:])\n",
    "    r=int(str(rm)[0])\n",
    "    \n",
    "    p2=dtrain[(dtrain.movId==m) & (dtrain.rating==r)][['movId','rating','userId']].sort_values(['rating','userId'])\n",
    "    p2=dtrain[dtrain.userId.isin(p2.userId)].groupby(by=['movId','rating'])[['userId']].count().reset_index()\n",
    "    p2.rename(columns={'userId':'nUsers'},inplace=True)\n",
    "    p2['ratMovId']=(p2.rating.astype(str)+p2.movId.astype(str))\n",
    "    \n",
    "    nbP2.loc[rm,list(p2['ratMovId'])]=p2.nUsers.to_numpy()\n",
    "\n",
    "nbP2=(nbP2+alpha).div(np.diag(np.array(nbP2))+l*alpha,axis=0)\n",
    "display(nbP2)\n",
    "end=time.time()\n",
    "print('\\nNaive Bayes DataFrame computation time (secs):',np.round(end-start,1))\n",
    "\n",
    "preds=[]\n",
    "for u, ms, mp in zip(usersdiv['ncf_users'], movies_seen, movies_pred):\n",
    "    p1=nbP1[nbP1.index.isin(mp)]\n",
    "    \n",
    "    display(p1)\n",
    "        \n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62313cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNWithMeans, Reader, Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Creation of the dataframe. Column names are irrelevant.\n",
    "\n",
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(ratings_v1[[\"userId\", \"movId\", \"rating\"]], reader)\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.05)\n",
    "\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "sim_options = {\n",
    "    \"name\": \"pearson\",\n",
    "    \"user_based\": True,  # compute  similarities between users\n",
    "    'min_support':35\n",
    "}\n",
    "algo = KNNWithMeans(min_k=30,sim_options=sim_options)\n",
    "\n",
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(algo, data, measures=[\"RMSE\", \"MAE\",\"FCP\"], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Greed Search\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#nnei=4                            # number of neighborhoods (user is in its own neighborhood)\n",
    "#min_nei=3                         # minimum number of neighborhoods\n",
    "#ncm=5                             # number of common items (movies)\n",
    "#min_sim=0.6                       # minimum similarity between users\n",
    "\n",
    "gsearch=[]\n",
    "\n",
    "for min_nei in [35,40,45]:\n",
    "    for ncm in [35,37,40,43,45]:\n",
    "        for min_sim in [0.3]:\n",
    "            nnei=min_nei+1\n",
    "\n",
    "\n",
    "            #Split in training and test\n",
    "            ptest=2.5/100\n",
    "            dtest=ratings_v1.sample(n=int(ptest*ratings_v1.shape[0]), replace=False,random_state=123456)\n",
    "            dtest.sort_index(inplace=True)\n",
    "\n",
    "            #Split in training and validation\n",
    "            dtrain=ratings_v1[~(ratings_v1.index.isin(dtest.index))]\n",
    "            pvald=2.5/100\n",
    "            dvald=dtrain.sample(n=int(pvald*ratings_v1.shape[0]), replace=False,random_state=1234)\n",
    "            dvald.sort_index(inplace=True)\n",
    "            dtrain=dtrain[~(dtrain.index.isin(dvald.index))]\n",
    "\n",
    "            aux=ratings_v1.groupby('movId').userId.count().reset_index().merge(\n",
    "                dtrain.groupby('movId').userId.count().reset_index(), how='left', on='movId')\n",
    "            #print('Which movie contributes the most in the test dataset? or\\nthe highest % of ratings used from a certain movie in the test dataset is:',\n",
    "            #      np.round(100-(aux.userId_y/aux.userId_x*100).min(),1),'%')\n",
    "            aux=ratings_v1.groupby('userId').movId.count().reset_index().merge(\n",
    "                dtrain.groupby('userId').movId.count().reset_index(), how='left', on='userId')\n",
    "            #print('Which user contributes the most in the test dataset? or\\nthe highest % of ratings used from a certain user in the test dataset is:',\n",
    "            #      np.round(100-(aux.movId_y/aux.movId_x*100).min(),1),'%')\n",
    "\n",
    "\n",
    "            #parameters\n",
    "\n",
    "            nu=ratings_v1.userId.nunique()    # number of users\n",
    "            nm=ratings_v1.movId.nunique()     # number of movies\n",
    "\n",
    "\n",
    "            #rating matrix\n",
    "            rmatrix = dtrain.set_index(['userId','movId']).rating.unstack()\n",
    "            rmatrix_n=np.array(rmatrix)-np.nanmean(np.matrix(rmatrix),axis=1)\n",
    "            #change np.matrix to np.array ABOVE\n",
    "            #ANOTHER CHANGE: add the mean rating per user once(AFTER) the predictions are obtained\n",
    "\n",
    "            #similarity matrix\n",
    "            start = time.time()\n",
    "            sim = np.transpose(pd.DataFrame(rmatrix_n)).corr(method='pearson',\n",
    "                                                             min_periods=ncm)\n",
    "            sim.fillna(-1,inplace=True)\n",
    "            sim=np.array(sim)\n",
    "            end = time.time()\n",
    "            print(end-start)\n",
    "\n",
    "            #Collaborative filtering\n",
    "\n",
    "            #get neighborhoods\n",
    "            start=time.time()\n",
    "            viz=[[np.sort(np.argpartition(sim[u], -nnei)[-nnei:]),\n",
    "                  sim[u][np.sort(np.argpartition(sim[u], -nnei)[-nnei:])]]\n",
    "\n",
    "                 for u in range(nu)]\n",
    "                #print(sim_train[0][viz[0]])\n",
    "\n",
    "            #exclude non correlated neighbors\n",
    "            viz=[viz[u][0][np.where(viz[u][1]>min_sim)]\n",
    "\n",
    "                 for u in range(nu)]\n",
    "\n",
    "            #exclude the user from its neighborhood\n",
    "            viz=[np.sort(list(set(viz[u])-set([u]))) \n",
    "\n",
    "                 for u in range(nu) ]\n",
    "                #print(sim_train[0][viz[0]])\n",
    "\n",
    "            end=time.time()\n",
    "            print('Neighborhood computation time:\\n',end-start)\n",
    "\n",
    "            #movies to predict\n",
    "            cf_users=np.where(np.array([len(v) for v in viz])>=min_nei)[0]\n",
    "            cb_users=list(set(range(nu))-set(cf_users))\n",
    "\n",
    "            start=time.time()\n",
    "            mov_pred=[np.sort(list(set(np.where((~np.isnan(rmatrix_n[viz[u],:])).sum(axis=0)!=0)[-1]) -\n",
    "                                   set(np.where(~np.isnan(rmatrix_n[u,:]))[-1]) \n",
    "                                  ))\n",
    "                      for u in cf_users]\n",
    "            end=time.time()\n",
    "            print('\\nMovies to predict computation time:\\n',end-start)\n",
    "\n",
    "            #Predict ratings and get top nri recommendations\n",
    "            start=time.time()\n",
    "            pred=[]\n",
    "\n",
    "            rmatrix_n0=np.copy(rmatrix_n)\n",
    "            rmatrix_n0[np.isnan(rmatrix_n0)] = 0\n",
    "            for u,v in zip(cf_users, range(len(mov_pred))):\n",
    "\n",
    "                rmatrix_n_neiu_predm=rmatrix_n0[viz[u]][:,mov_pred[v]]\n",
    "                sim_u_neiu=sim[u,viz[u]]\n",
    "\n",
    "                pred_u=np.array(np.matmul(sim_u_neiu,\n",
    "                                          rmatrix_n_neiu_predm)/np.matmul(np.abs(sim_u_neiu),\n",
    "                                                                          np.array(~np.isnan(rmatrix_n[viz[u]][:,mov_pred[v]]))\n",
    "                                                                         )\n",
    "                               ) #+ np.nanmean(np.array(rmatrix)[u])\n",
    "\n",
    "                #indexes of movies with greatest ranking from mov_pred[0]\n",
    "                \"\"\"\n",
    "\n",
    "                movie_u=np.argpartition(pred_u, -nri)[-nri:]\n",
    "                movie_u=mov_pred[v][movie_u]\n",
    "\n",
    "                pred.append([movie_u,\n",
    "                             pred_u[np.argpartition(pred_u, -nri)[-nri:]]\n",
    "                            ])\n",
    "\n",
    "                Instead of recommending top nri movies, lets see ALL the potencial movies and ratings\n",
    "                later we can recommend top nri movies\n",
    "                \"\"\"\n",
    "                pred.append(pred_u)\n",
    "\n",
    "            w=np.array(np.copy(rmatrix))\n",
    "            predd=[p+np.nanmean(w[u]) for p,u in zip(pred,cf_users)]\n",
    "\n",
    "            end=time.time()\n",
    "            print('\\nPrediction computation time:\\n', end-start)\n",
    "\n",
    "            ratingsPred=[]\n",
    "            for ui in range(len(cf_users)):\n",
    "                u=cf_users[ui]\n",
    "                for mp,mr in zip(mov_pred[ui],predd[ui]):\n",
    "                    ratingsPred.append([u,mp,mr])\n",
    "            ratingsPred_v=pd.DataFrame(ratingsPred,\n",
    "                                       columns=['userId','movId','ratingPred']).merge(dvald[['userId','movId','rating']],how='inner',on=['userId','movId'])\n",
    "            print('Normalized RMSE (NRMSE):\\n',\n",
    "                  np.round(\n",
    "                      100*np.sqrt(np.sum((ratingsPred_v.ratingPred-ratingsPred_v.rating)**2)/ratingsPred_v.shape[0])/5 , 1) )\n",
    "            print('Normalized MAE (NMAE):\\n',\n",
    "                  np.round(\n",
    "                      100*np.sqrt(np.sum(np.abs(ratingsPred_v.ratingPred-ratingsPred_v.rating))/ratingsPred_v.shape[0])/5, 1) )\n",
    "            print('Hit Rate:\\n',\n",
    "                  np.round(ratingsPred_v.shape[0]/dvald.shape[0]*100,1))\n",
    "            gsearch.append(\n",
    "                [min_nei,ncm,min_sim,np.round(ratingsPred_v.shape[0]/dvald.shape[0]*100,1) ])\n",
    "            print([min_nei,ncm,min_sim,np.round(ratingsPred_v.shape[0]/dvald.shape[0]*100,1) ])\n",
    "            print('\\n'*3)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assess evaluation metrics\n",
    "\n",
    "#for each user:\n",
    "filtrando através das categorias das recomendações qual a percentagem\n",
    "das categorias dos filmes ja vistos para esse mesmo user?\n",
    "E atores?\n",
    "\n",
    "\"\"\"\n",
    "dtest[dtest.userId.isin(cf_users)].shape[0]\n",
    "\n",
    "\n",
    "mov_pred[0]\n",
    "pred[0]\n",
    "\n",
    "aa=[]\n",
    "for u,ui in zip(cf_users,range(len(cf_users))):\n",
    "    \n",
    "    t3_mi=np.argpartition(pred[ui], -5)[-5:]\n",
    "    \n",
    "    t3_mr=pred[ui][t3_mi]\n",
    "    t3_mo=mov_pred[ui][t3_mi]\n",
    "    \n",
    "    for m,mr in zip(t3_mo,t3_mr):\n",
    "        aa.append([u,m,mr])\n",
    "aa=pd.DataFrame(aa,columns=['userId','movId','rat_pred']).sort_values(by=['userId','movId'])\n",
    "aa\n",
    "\n",
    "c=dtest[dtest.userId.isin(cf_users)].merge(aa,how='left',on=['userId','movId'])\n",
    "display(c)\n",
    "display(c[~np.isnan(c.rat_pred)])\n",
    "#numero users com recomendaçoes vistas/n users nos tests\n",
    "print('hit rate  (%) : ',c[~np.isnan(c.rat_pred)].userId.nunique()/c.userId.nunique()*100)\n",
    "print('precision (%) : ',\n",
    "      100-100*np.mean(np.abs(c[~np.isnan(c.rat_pred)].rating-c[~np.isnan(c.rat_pred)].rat_pred))/(ratings.rating.max()-ratings.rating.min())\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d717973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagging matrix\n",
    "tmatrix = tags[~tags.duplicated(subset=['userId','movieId'])]\n",
    "tmatrix['tag']=1\n",
    "tmatrix = tmatrix.set_index(['userId','movieId']).tag.unstack()\n",
    "tmatrix.fillna(0, inplace=True)\n",
    "\n",
    "addcols = list(set(ratings.movieId.unique()) - set(tmatrix.columns))\n",
    "addcols_df = pd.DataFrame(np.zeros((tmatrix.shape[0],len(addcols))))\n",
    "addcols_df.columns = addcols\n",
    "addcols_df.index = tmatrix.index\n",
    "tmatrix = pd.concat([tmatrix,addcols_df ], axis=1)\n",
    "\n",
    "addrows = list(set(ratings.userId.unique()) - set(tmatrix.index))\n",
    "addrows_df = pd.DataFrame(np.zeros((len(addrows),tmatrix.shape[1])))\n",
    "addrows_df.columns = tmatrix.columns\n",
    "addrows_df.index = addrows\n",
    "tmatrix = pd.concat([tmatrix,addrows_df ], axis=0)\n",
    "\n",
    "tmatrix.sort_index(axis=0,inplace=True)\n",
    "tmatrix.sort_index(axis=1,inplace=True)\n",
    "\n",
    "#genre matrix\n",
    "gmatrix=movies[['movieId','genres']]\n",
    "gmatrix=gmatrix.set_index(['movieId']).genres.str.split(pat='|', expand=True).reset_index()\n",
    "gmatrix=pd.melt(gmatrix, id_vars='movieId',value_vars=gmatrix.columns)\n",
    "gmatrix=gmatrix[~gmatrix['value'].isin([None])].sort_values(by='movieId')\n",
    "gmatrix['variable']=1\n",
    "gmatrix = gmatrix.set_index(['movieId','value']).variable.unstack().fillna(0)\n",
    "gmatrix = gmatrix[gmatrix.index.isin(np.unique(ratings.movieId))]\n",
    "\n",
    "\n",
    "\n",
    "aux=pd.DataFrame(ratings.groupby(by='movId').userId.count().sort_values(ascending=True))\n",
    "display((aux.userId>=16).index)\n",
    "aux=aux[aux.userId>=16]\n",
    "\n",
    "ptest=2.5/100\n",
    "dtest=ratings[ratings.movId.isin(aux.index)].sample(frac=ptest, replace=False,random_state=123)\n",
    "dtest.sort_index(inplace=True)\n",
    "dtrain=ratings[~(ratings.index.isin(dtest.index))]\n",
    "#print(dtrain.userId.nunique(),dtrain.movId.nunique())\n",
    "\n",
    "####################\n",
    "print(ratings.groupby('userId').movId.count().describe())\n",
    "\n",
    "b=pd.DataFrame(dtest.groupby('movId').userId.count().sort_values())\n",
    "display(b)\n",
    "c=b.merge(\n",
    "    ratings.groupby('movId').userId.count().sort_values(),how='left',\n",
    "    left_on=b.index,right_on=ratings.groupby('movId').userId.count().sort_values().index)\n",
    "c['dif']=(c.userId_y-c.userId_x)/c.userId_y*100\n",
    "display(c.sort_values(by='dif',ascending=True))\n",
    "#display(ratings.groupby('userId').movId.count().head(58))\n",
    "\n",
    "\n",
    "print(ratings.groupby('movId').userId.count().describe())\n",
    "\n",
    "b=pd.DataFrame(dtest.groupby('userId').movId.count().sort_values())\n",
    "display(b)\n",
    "c=b.merge(\n",
    "    ratings.groupby('userId').movId.count().sort_values(),how='left',\n",
    "    left_on=b.index,right_on=ratings.groupby('userId').movId.count().sort_values().index)\n",
    "c['dif']=(c.movId_y-c.movId_x)/c.movId_y*100\n",
    "display(c.sort_values(by='dif',ascending=True))\n",
    "\n",
    "\n",
    "\n",
    "aux=pd.DataFrame(ratings.groupby(by='movId').userId.count().sort_values(ascending=True))\n",
    "display((aux.userId>=16).index)\n",
    "aux=aux[aux.userId>=16]\n",
    "\n",
    "ptest=2.5/100\n",
    "dtest=ratings[ratings.movId.isin(aux.index)].sample(frac=ptest, replace=False,random_state=123)\n",
    "dtest.sort_index(inplace=True)\n",
    "dtrain=ratings[~(ratings.index.isin(dtest.index))]\n",
    "#print(dtrain.userId.nunique(),dtrain.movId.nunique())\n",
    "\n",
    "####################\n",
    "print(ratings.groupby('userId').movId.count().describe())\n",
    "\n",
    "b=pd.DataFrame(dtest.groupby('movId').userId.count().sort_values())\n",
    "display(b)\n",
    "c=b.merge(\n",
    "    ratings.groupby('movId').userId.count().sort_values(),how='left',\n",
    "    left_on=b.index,right_on=ratings.groupby('movId').userId.count().sort_values().index)\n",
    "c['dif']=(c.userId_y-c.userId_x)/c.userId_y*100\n",
    "display(c.sort_values(by='dif',ascending=True))\n",
    "#display(ratings.groupby('userId').movId.count().head(58))\n",
    "\n",
    "\n",
    "print(ratings.groupby('movId').userId.count().describe())\n",
    "\n",
    "b=pd.DataFrame(dtest.groupby('userId').movId.count().sort_values())\n",
    "display(b)\n",
    "c=b.merge(\n",
    "    ratings.groupby('userId').movId.count().sort_values(),how='left',\n",
    "    left_on=b.index,right_on=ratings.groupby('userId').movId.count().sort_values().index)\n",
    "c['dif']=(c.movId_y-c.movId_x)/c.movId_y*100\n",
    "display(c.sort_values(by='dif',ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a0228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Training & Test Set\n",
    "\n",
    "user_test=np.sort(np.random.choice(range(nu),\n",
    "                                  int(nu*(1-train_per)),\n",
    "                                  replace=False)\n",
    "                )\n",
    "user_train=np.array(list(set(range(nu))-set(user_test)))\n",
    "\n",
    "print('Size of test set:',user_test.shape[0],'\\n\\nUsers:\\n',user_test)\n",
    "\n",
    "sim_train = sim[user_train][:,user_train]\n",
    "rmatrix_n_train = rmatrix_n[user_train,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5364fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collaborative filtering\n",
    "\n",
    "#get neighborhoods\n",
    "start=time.time()\n",
    "viz=[[np.sort(np.argpartition(sim_train[u], -nnei)[-nnei:]),\n",
    "      sim_train[u][np.sort(np.argpartition(sim_train[u], -nnei)[-nnei:])]\n",
    "     ]\n",
    "     \n",
    "     for u in range(user_train.shape[0])]\n",
    "    #print(sim_train[0][viz[0][0]])\n",
    "\n",
    "#exclude non correlated neighbors\n",
    "viz=[viz[u][0][np.where(viz[u][1]>min_sim)]\n",
    "     \n",
    "     for u in range(user_train.shape[0])]\n",
    "\n",
    "#exclude the user from its neighborhood\n",
    "viz=[np.sort(list(set(viz[u])-set([u]))) \n",
    "     \n",
    "     for u in range(user_train.shape[0]) ]\n",
    "    #print(sim_train[0][viz[0]])\n",
    "    \n",
    "end=time.time()\n",
    "print('Neighborhood computation time:\\n',end-start)\n",
    "\n",
    "#movies to predict\n",
    "cf_users=np.where(np.array([len(v) for v in viz])>=min_nei)[0]          #index!\n",
    "cb_users=np.sort(list(set(user_train)-set(user_train[cf_users])))       #users\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "mov_pred=[np.sort(list(set(np.where(np.nansum(rmatrix_n_train[viz[u],:],axis=0)!=0)[-1]) -\n",
    "                       set(np.where(~np.isnan(rmatrix_n_train[u,:]))[-1]) \n",
    "                      ))\n",
    "          for u in cf_users]\n",
    "end=time.time()\n",
    "print('\\nMovies to predict computation time:\\n',end-start)\n",
    "\n",
    "#Predict ratings and get top nri recommendations\n",
    "start=time.time()\n",
    "pred=[]\n",
    "\n",
    "rmatrix_n_train0=np.copy(rmatrix_n_train)\n",
    "rmatrix_n_train0[np.isnan(rmatrix_n_train0)] = 0\n",
    "for u,v in zip(cf_users, range(len(mov_pred))):\n",
    "    \n",
    "    rmatrix_n_neiu_predm=rmatrix_n_train0[viz[u]][:,mov_pred[v]]\n",
    "    sim_u_neiu=sim_train[u,viz[u]]\n",
    "\n",
    "    pred_u=np.array(np.matmul(sim_u_neiu,\n",
    "                              rmatrix_n_neiu_predm)/np.matmul(np.abs(sim_u_neiu),\n",
    "                                                              rmatrix_n_neiu_predm!=0))\n",
    "\n",
    "    #indexes of movies with greatest ranking from mov_pred[0]\n",
    "    movie_u=np.argpartition(pred_u, -nri)[-nri:]\n",
    "    movie_u=mov_pred[v][movie_u]\n",
    "    \n",
    "    pred.append([movie_u,\n",
    "                 pred_u[np.argpartition(pred_u, -nri)[-nri:]]\n",
    "                ])\n",
    "end=time.time()\n",
    "print('\\nPrediction computation time:\\n', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ec871",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid=int(input('Predict recommendation to user Id: '))\n",
    "while uid in user_test:\n",
    "    uid=int(input('\\nUser Id used to test\\nPredict recommendation to user Id: '))\n",
    "\n",
    "id_train=np.where(user_train==uid)[0][0]\n",
    "print('id_train:',id_train)\n",
    "if len(list(viz[id_train])) >=min_nei:\n",
    "    print('\\nNeighbors:\\n',list(user_train[viz[id_train]]))\n",
    "\n",
    "    print('\\nPredicted movies:\\n')\n",
    "    id_new=np.where(user_train[cf_users]==uid)[0][0]\n",
    "    print('id_new: ',id_new)\n",
    "    df=pd.DataFrame(np.transpose(pred[id_new]),\n",
    "                   columns=['mov_id','prediction'])\n",
    "    df.mov_id=df.mov_id.astype('int64')\n",
    "    df.sort_values(by='prediction',ascending=False,inplace=True)\n",
    "    df.index=[1,2,3,4,5]\n",
    "    display(df.merge(movies,on='mov_id'))\n",
    "\n",
    "    print('\\n'*2,'*'*20,'\\n')\n",
    "    display(ratings[ratings.userId==uid+1].merge(movies,\n",
    "                                                 left_on='movieId', right_on='movieId').head(10))\n",
    "else:\n",
    "    print('\\nsmall neighborhood: ',len(list(viz[id_train])),'\\nNo prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e07e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set - Error\n",
    "\n",
    "#get neighborhoods\n",
    "start=time.time()\n",
    "vizt=[np.sort(np.argpartition(sim[u], -nnei)[-nnei:])\n",
    "      for u in user_test]\n",
    "end=time.time()\n",
    "print('Neighborhood computation time:\\n',end-start)\n",
    "    #print(vizt[0])\n",
    "\n",
    "#exclude the user from its neighborhood\n",
    "vizt=[np.sort(list(set(vizt[v])-set([u])))\n",
    "      for u,v in zip(user_test,range(user_test.shape[0])) ]\n",
    "\n",
    "    #print(vizt[0])\n",
    "\n",
    "#movies to predict\n",
    "start=time.time()\n",
    "mov_pred=[np.where(~np.isnan(rmatrix_n[u]))[0]\n",
    "          for u in user_test]\n",
    "end=time.time()\n",
    "print('\\nMovies to predict computation time:\\n',end-start)\n",
    "    #print(mov_pred[0])\n",
    "\n",
    "    #display(ratings[ratings.userId.isin(user_test+1)].sort_values(['userId','movieId']).head(30))\n",
    "\n",
    "#Predict ratings and get top nri recommendations\n",
    "start=time.time()\n",
    "rated=np.sort(ratings.movieId.unique())\n",
    "pred=[]\n",
    "\n",
    "rmatrix_n_t=np.copy(rmatrix_n)\n",
    "rmatrix_n_t[np.isnan(rmatrix_n_t)] = 0\n",
    "for u,v in zip(user_test,range(len(vizt)) ):\n",
    "    \n",
    "    rmatrix_n_neiu_predm=rmatrix_n_t[vizt[v]][:,mov_pred[v]]\n",
    "    sim_u_neiu=sim[u,vizt[v]]\n",
    "\n",
    "    pred_u=np.array(np.matmul(sim_u_neiu,\n",
    "                              rmatrix_n_neiu_predm)/np.matmul(np.abs(sim_u_neiu),\n",
    "                                                              ~np.isnan(rmatrix_n[vizt[v]][:,mov_pred[v]]))\n",
    "                   )\n",
    "    \n",
    "    pred.append(pred_u)\n",
    "end=time.time()\n",
    "print('\\nPrediction computation time:\\n', end-start)\n",
    "\n",
    "print('\\nMSE:')\n",
    "mse_n=0\n",
    "mse=0\n",
    "for u,v in zip(user_test,range(len(pred))):\n",
    "    \n",
    "    error=(rmatrix_n[u][:,mov_pred[v]]-pred[v])\n",
    "    error= np.array(error)**2\n",
    "    error=error[~np.isnan(error)]\n",
    "    \n",
    "    mse_n=mse_n+error.shape[0]\n",
    "    mse=mse+np.sum(error)\n",
    "print(mse/mse_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rascunho\n",
    "\n",
    "#np.where(rmatrix.columns==2)[0].tolist()[0]\n",
    "mov_pred[0]\n",
    "#rmatrix_n[0,:][:,np.where(rmatrix_n[0,:]!=0)[1]]\n",
    "#rmatrix_n[0,:]\n",
    "\n",
    "#predict ratings\n",
    "\n",
    "#def prev_rat(neigh,ratings):\n",
    "np.matrix(rmatrix)[:,mov_pred[0]][j[0],]\n",
    "a=np.matrix([[2,np.nan,3,np.nan],\n",
    "             [np.nan,3,3,np.nan],\n",
    "             [5,4,np.nan,4],\n",
    "             [np.nan,2,5,np.nan]])\n",
    "#print(a)\n",
    "a[np.isnan(a)] = 0\n",
    "print(a)\n",
    "print(np.matmul(np.matrix([-1,1,0.5,1]),a))\n",
    "\n",
    "print(a!=0)\n",
    "print(np.matmul(np.matrix([-1,1,0.5,1]),a!=0))\n",
    "\n",
    "print('\\n'*2)\n",
    "print(np.matmul(np.matrix([-1,1,0.5,1]),a)/np.matmul(np.matrix([-1,1,0.5,1]),a!=0))\n",
    "\n",
    "print(\"\\n\"*2)\n",
    "print(\"~\"*20)\n",
    "print(\"\\n\"*3)\n",
    "\n",
    "#verficar que os filmes/items selecionados sao realmente os items e nao as posiçoes \n",
    "\n",
    "rmatrix_u1_items_norm=rmatrix_n[j[0]][:,mov_pred[0]]\n",
    "rmatrix_u1_items_norm[np.isnan(rmatrix_u1_items_norm)]=0\n",
    "sim_u1=np.matrix(sim)[0,j[0]]\n",
    "\n",
    "\"\"\"\n",
    "Movies Prediction list\n",
    "\n",
    "It is missing\n",
    "+np.nanmean(np.matrix(rmatrix)[0]))\n",
    "\n",
    "\"\"\"\n",
    "k=np.array(np.matmul(sim_u1,\n",
    "                     rmatrix_u1_items_norm)/np.matmul(np.abs(sim_u1),\n",
    "                                                      rmatrix_u1_items_norm!=0))\n",
    "k=k[0]\n",
    "\n",
    "print(np.max(k))\n",
    "print(np.where(k==np.max(k))[0])\n",
    "print(k[np.where(k==np.max(k))[0]])\n",
    "\n",
    "print('holla')\n",
    "print(k.argsort()[-5:])\n",
    "print(k[k.argsort()[-5:]])\n",
    "\n",
    "#movie_u1=np.argmax(mov_pred[0])\n",
    "\n",
    "movie_u1=k.argsort()[-5:]\n",
    "#+ np.nanmean(np.matrix(rmatrix)[0]))]\n",
    "#print(movie_u1)\n",
    "#print('ola')\n",
    "#print(k[movie_u1])\n",
    "#rmatrix.columns[movie_u1]\n",
    "\n",
    "rated=np.sort(ratings.movieId.unique())\n",
    "#print(rated)\n",
    "rated[k.argsort()[-5:]]\n",
    "\n",
    "print(np.where(np.array([len(v) for v in viz])>=min_nei)[0],'\\n'*2)\n",
    "for u in np.where(np.array([len(v) for v in viz])>=min_nei)[0]:\n",
    "    print(viz[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificar que para o primeiro user o item a recomendar é o item 895\n",
    "a1=np.sort(np.argsort(np.matrix(sim.fillna(-1))[0])[0,-10:])\n",
    "a1=np.sort(list(set(a1.tolist()[0])-set([0])))\n",
    "print(a1)\n",
    "print('\\n'*3)\n",
    "\n",
    "movies_to_pred_u1 =np.where(np.sum(np.matrix(rmatrix.fillna(0))[j[0],:],axis=0)>0)[1]\n",
    "print(movies_to_pred_u1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f37c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "\n",
    "display(ratings.head())\n",
    "print('Num unique users: ' ,ratings.userId.nunique() )\n",
    "print('Num unique movies: ',ratings.movieId.nunique())\n",
    "print('\\n')\n",
    "print(ratings.rating.describe())\n",
    "\n",
    "ratings.groupby(by='userId').movieId.agg('nunique').describe()\n",
    "dfr=pd.DataFrame(ratings.groupby(by='movieId').rating.agg('mean'))\n",
    "dfr[(dfr.rating >=4) & (dfr.rating<=5)].shape\n",
    "dfr1=pd.DataFrame(ratings.groupby(by='movieId').userId.agg('nunique'))\n",
    "dfr1[dfr1.userId>=5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199fa2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#content based RS\n",
    "\"\"\"\n",
    "user_cat_matrix = [\n",
    "    np.multiply(\n",
    "        np.tile(np.matrix(rmatrix.fillna(0))[i,:].T, (1, 20)),\n",
    "        np.matrix(gmatrix)\n",
    "    )\n",
    "    for i in range(610)\n",
    "]\n",
    "\n",
    "user_cat_vect = [\n",
    "    np.sum(user_cat,axis=0)/np.sum(np.sum(user_cat,axis=0))\n",
    "    for user_cat in user_cat_matrix\n",
    "]\n",
    "\n",
    "pred_rat=[\n",
    "    np.sum(\n",
    "        np.multiply(\n",
    "            np.tile(user_cat_vect[u], (9724, 1)) ,\n",
    "            np.matrix(gmatrix)\n",
    "        ),\n",
    "        axis=1)\n",
    "    for u in range(610)\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "#faster\n",
    "x1=np.matmul(np.matrix(rmatrix.fillna(0)),\n",
    "          np.matrix(gmatrix)\n",
    "         )\n",
    "x1=normalize(x1,axis=1,norm='l1')\n",
    "\n",
    "x2=np.matmul(x1,np.matrix(gmatrix).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c943e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model = NearestNeighbors(n_neighbors=25,\n",
    "                         metric='cosine',\n",
    "                         algorithm='brute',\n",
    "                         n_jobs=-1)\n",
    "model.fit(rmatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab64ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "u=0\n",
    "np.sort(list(set(np.where(np.sum(rmatrix_n_train[viz[u],:],axis=0)!=0)[1]) -\n",
    "                       set(np.where(rmatrix_n_train[u,:]!=0)[1]) \n",
    "                      ))\n",
    "#Collaborative filtering\n",
    "\n",
    "#get neighborhoods\n",
    "start=time.time()\n",
    "viz=[np.sort(np.argpartition(sim_train[u], -nnei)[-nnei:])\n",
    "     for u in range(user_train.shape[0])]\n",
    "end=time.time()\n",
    "\n",
    "print('Neighborhood computation time:\\n',end-start)\n",
    "\n",
    "#exclude the user from its neighborhood\n",
    "viz=[np.sort(list(set(viz[u])-set([u]))) \n",
    "     for u in range(user_train.shape[0]) ]\n",
    "\n",
    "#movies to predict\n",
    "start=time.time()\n",
    "mov_pred=[np.sort(list(set(np.where(np.sum(rmatrix_n_train[viz[u],:],axis=0)!=0)[1]) -\n",
    "                       set(np.where(rmatrix_n_train[u,:]!=0)[1]) \n",
    "                      ))\n",
    "          for u in range(user_train.shape[0])]\n",
    "end=time.time()\n",
    "print('\\nMovies to predict computation time:\\n',end-start)\n",
    "\n",
    "#Predict ratings and get top nri recommendations\n",
    "start=time.time()\n",
    "rated=np.sort(ratings.movieId.unique())\n",
    "pred=[]\n",
    "\n",
    "for u in range(user_train.shape[0]):\n",
    "    \n",
    "    rmatrix_n_neiu_predm=rmatrix_n_train[viz[u]][:,mov_pred[u]]\n",
    "    sim_u_neiu=sim_train[u,viz[u]]\n",
    "\n",
    "    pred_u=np.array(np.matmul(sim_u_neiu,\n",
    "                              rmatrix_n_neiu_predm)/np.matmul(np.abs(sim_u_neiu),\n",
    "                                                              rmatrix_n_neiu_predm!=0))\n",
    "    \n",
    "    pred_u=pred_u[0]\n",
    "    #indexes of movies with greatest ranking from mov_pred[0]\n",
    "    movie_u=np.argpartition(pred_u, -nri)[-nri:]\n",
    "    movie_u=mov_pred[u][movie_u]\n",
    "\n",
    "    pred.append([rated[movie_u],\n",
    "                 pred_u[np.argpartition(pred_u, -5)[-5:]]\n",
    "                ])\n",
    "end=time.time()\n",
    "print('\\nPrediction computation time:\\n', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e67280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e743f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f169c7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
